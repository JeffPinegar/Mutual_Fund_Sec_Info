{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scape Sector information for multiple Mutual fund from Yahoo Finance\n",
    "\n",
    "This works using splinter to open the url in the browser.  We must do it this way because the data we are looking for is not it the HTML if look at it outside of a browser. The data is loaded with the webpage by a java script.  So if we just did a \"requests.get(url)\" the data we are after would not be there.  We must \n",
    "load a browser and scrape/pars that.\n",
    "\n",
    "### Next Steps in Python\n",
    "* it is not going to the next mutual fund\n",
    "* Convert the values in the data frame from strings to values\n",
    "* Export that data frame as a CSV file\n",
    "\n",
    "### Next step in Excel \n",
    "* Import CSV file in my financial spreadsheet\n",
    "* Report my exposer to each of these sectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website:  https://finance.yahoo.com/quote/VTSAX/holdings?p=VTSAX\n",
    "\n",
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# DESCRIPTION:  This function is used to splits a string into 2 parts at the first number. \n",
    "#               A dictionary is created with the first string as the key and the second string as the value\n",
    "# ARGUMENTS:  A string that starts with a string and is followed by a value.\n",
    "# RETURN:  Returns a dictionary where the letters in the string form the key and the numbers become the values of the dictionary entry.\n",
    "\n",
    "def split_string_at_first_number(string):\n",
    "    index = len(string)\n",
    "\n",
    "    # count the characters until you find the first number.\n",
    "    for i, char in enumerate(string):\n",
    "        \n",
    "        # break out of the for loop when you find a digit.\n",
    "        if char.isdigit():\n",
    "            index = i\n",
    "            break\n",
    "\n",
    "    # divide the string in to the key and the value based on the location of the first digit.    \n",
    "    key = string[:index]\n",
    "    value = string[index:]\n",
    "\n",
    "    # Put the key and value into a dictionary and return the dictionary\n",
    "    dictionary = {key: value}\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://finance.yahoo.com/quote/BDBKX/performance?p=BDBKX\n",
      "dict_1_Month\n",
      "{'1-Month': '0.75%'}\n",
      "dict_3_Month\n",
      "{'3-Month': '-7.23%'}\n",
      "dict_1_Year\n",
      "{'1-Year': '-9.18%'}\n",
      "dict_3_Year\n",
      "{'3-Year': '13.73%'}\n",
      "dict_5_Year\n",
      "{'5-Year': '3.82%'}\n",
      "dict_10_Year\n",
      "{'10-Year': '7.87%'}\n"
     ]
    }
   ],
   "source": [
    "mutual_fund = 'BDBKX'\n",
    "url_part_1 = 'https://finance.yahoo.com/quote/'\n",
    "url_part_2 = '/performance?p='\n",
    "\n",
    "# Build the full url to be scrapped.\n",
    "url = url_part_1 + mutual_fund + url_part_2 + mutual_fund\n",
    "\n",
    "# Print the full URL - this is done only for diagnostic purposes\n",
    "print(url)\n",
    "\n",
    "# open a browser to the required URL\n",
    "browser.visit(url)\n",
    "\n",
    "# Create a Beautiful Soup object\n",
    "# This will open the url in the browser.  We must do it this way because the data we are looking for is not it the HTML if look at it outside of a browser.\n",
    "# The data is loaded with the webpage by a java script.  So if we just did a \"requests.get(url)\" the data we are after would not be there.  We must \n",
    "# load a browser and scrape/pars that.\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Start a dictionary with the key-value pair for the fund name.\n",
    "#Fund_Returs ={'Fund_Code': fund}\n",
    "\n",
    "# get the industry sector information from the url\n",
    "# The search strings come from \"inspecting\" the website where the data is located.\n",
    "# The website does not unquicly identify sector information. It get extra information that will need to be cleaned out \n",
    "\n",
    "# Ticker_returns = soup.find_all('div', class_='Bdbw(1px) Bdbc($seperatorColor) Bdbs(s) H(25px) Pt(10px)')\n",
    "#Ticker_returns = soup.find_all('div', class_=\"Mend(5px) Whs(nw)\")\n",
    "\n",
    "# # Find the div with the class \"Mb(25px)\"\n",
    "# div = soup.find('div', class_='Mb(25px)')\n",
    "\n",
    "# # Find the div with the class \"Bdbw(1px)\" inside the previous div\n",
    "# divs = div.find_all('div', class_='Bdbw(1px)')\n",
    "\n",
    "# # Extract the 1-month and 3-month values and store them in dictionaries\n",
    "# one_month = {'1-month': divs[2].find_all('span')[1].get_text()}\n",
    "# three_month = {'3-Month': divs[3].find_all('span')[1].get_text()}\n",
    "\n",
    "# # Print the dictionaries\n",
    "# print(one_month)\n",
    "# print(three_month)\n",
    "\n",
    "\n",
    "# result = {}\n",
    "\n",
    "# this is the html code \n",
    "# <span class=\"W(50%) D(b) Fl(start) Ta(s)\">\n",
    "#       <span class=\"Mend(5px) Whs(nw)\">\n",
    "#             <span>1-Month</span>\n",
    "#       </span>\n",
    "# </span>\n",
    "# <span class=\"W(20%) D(b) Fl(start) Ta(e)\"> 0.75%</span>\n",
    "# <span class=\"W(30%) D(b) Fl(start) Ta(e)\"> 0.02%</span>\n",
    "\n",
    "periods = ['1-Month',\n",
    "            '3-Month',\n",
    "            '1-Year',\n",
    "            '3-Year',\n",
    "            '5-Year',\n",
    "            '10-Year'\n",
    "            ]\n",
    "\n",
    "for period in periods:\n",
    "      period_return = soup.find('span', text=period).find_next('span').find_next('span').find_next('span').text\n",
    "      dict = str('dict_' + period.replace('-','_'))\n",
    "      print(dict)\n",
    "      dict = {period: period_return}\n",
    "      print(dict)\n",
    "\n",
    "\n",
    "# one_month_return = soup.find('span', text='1-Month').find_next('span').find_next('span').find_next('span').text\n",
    "# three_month_return = soup.find('span', text='3-Month').find_next('span').find_next('span').find_next('span').text\n",
    "# one_year_return = soup.find('span', text='1-Year').find_next('span').find_next('span').find_next('span').text\n",
    "# three_year_return = soup.find('span', text='3-Year').find_next('span').find_next('span').find_next('span').text\n",
    "# five_year_return = soup.find('span', text='5-Year').find_next('span').find_next('span').find_next('span').text\n",
    "# ten_year_return = soup.find('span', text='10-Year').find_next('span').find_next('span').find_next('span').text\n",
    "\n",
    "# one_month_dict = {'1-month': one_month_return}\n",
    "# three_month_dict = {'3-Month': three_month_return}\n",
    "# ten_year_dict = {'3-Month': ten_year_return}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_10_Year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dict_10_Year\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_10_Year' is not defined"
     ]
    }
   ],
   "source": [
    "dict_10_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dict_10-Year'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'dict_' + period\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Morningstar Return Rating': 'Morningstar Return Rating',\n",
       " 'Year-to-Date Return': 'Year-to-Date Return',\n",
       " '5-Year Average Return': '5-Year Average Return',\n",
       " 'Number of Years Up': 'Number of Years Up',\n",
       " 'Number of Years Down': 'Number of Years Down',\n",
       " 'Best 1 Yr Total Return (Apr 24, 2023)': 'Best 1 Yr Total Return (Apr 24, 2023)',\n",
       " 'Worst 1 Yr Total Return (Apr 24, 2023)': 'Worst 1 Yr Total Return (Apr 24, 2023)',\n",
       " 'Best 3-Yr Total Return': 'Best 3-Yr Total Return',\n",
       " 'Worst 3-Yr Total Return': 'Worst 3-Yr Total Return',\n",
       " 'YTD': 'YTD',\n",
       " '1-Month': '1-Month',\n",
       " '3-Month': '3-Month',\n",
       " '1-Year': '1-Year',\n",
       " '3-Year': '3-Year',\n",
       " '5-Year': '5-Year',\n",
       " '10-Year': '10-Year',\n",
       " 'Last Bull Market': 'Last Bull Market',\n",
       " 'Last Bear Market': 'Last Bear Market',\n",
       " '2023': '2.77%',\n",
       " '2022': '-7.47%',\n",
       " '2021': '12.65%',\n",
       " '2020': '-30.51%',\n",
       " '2019': '14.61%',\n",
       " '2018': '0.00%',\n",
       " '2017': '2.53%',\n",
       " '2016': '-1.43%',\n",
       " '2015': '4.45%',\n",
       " '2014': '1.08%',\n",
       " '2013': '12.38%',\n",
       " '2012': '12.42%',\n",
       " '2011': '-1.75%'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m div \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     span_list \u001b[39m=\u001b[39m div\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     key \u001b[39m=\u001b[39m span_list[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m      4\u001b[0m     value \u001b[39m=\u001b[39m span_list[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m      5\u001b[0m     result[key] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for div in soup.find_all('div'):\n",
    "    span_list = div.find_all('span')\n",
    "    key = span_list[0].text.strip()\n",
    "    value = span_list[1].text.strip()\n",
    "    result[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(span_list\u001b[39m.\u001b[39;49mtext)\n",
      "File \u001b[1;32mc:\\Users\\jspinega\\Anaconda3\\lib\\site-packages\\bs4\\element.py:2289\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   2288\u001b[0m     \u001b[39m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m   2290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mResultSet object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key\n\u001b[0;32m   2291\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "print(span_list.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# DESCRIPTION:  Find the industry sector loading of a mutual fund by scraping a Yahoo finance webpage\n",
    "# ARGUMENTS:  The mutual fund ticker symbol\n",
    "# RETURN:  a data frame containing the industry sector loading of that fund\n",
    "\n",
    "def one_return(mutual_fund):\n",
    "      # build the URL where the data is located.\n",
    "      # mutual_fund is the ticker symble of the fund to be scraped\n",
    "      # Constant pieces of the Yahoo finance website to be scraped\n",
    "      url_part_1 = 'https://finance.yahoo.com/quote/'\n",
    "      url_part_2 = '/performance?p='\n",
    "      \n",
    "      # Build the full url to be scrapped.\n",
    "      url = url_part_1 + mutual_fund + url_part_2 + mutual_fund\n",
    "\n",
    "      # Print the full URL - this is done only for diagnostic purposes\n",
    "      print(url)\n",
    "\n",
    "      # open a browser to the required URL\n",
    "      browser.visit(url)\n",
    "\n",
    "      # Create a Beautiful Soup object\n",
    "      # This will open the url in the browser.  We must do it this way because the data we are looking for is not it the HTML if look at it outside of a browser.\n",
    "      # The data is loaded with the webpage by a java script.  So if we just did a \"requests.get(url)\" the data we are after would not be there.  We must \n",
    "      # load a browser and scrape/pars that.\n",
    "\n",
    "      html = browser.html\n",
    "      soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "      # Start a dictionary with the key-value pair for the fund name.\n",
    "      Industry_sec ={'Fund_Code': fund}\n",
    "\n",
    "      # get the industry sector information from the url\n",
    "      # The search strings come from \"inspecting\" the website where the data is located.\n",
    "      # The website does not unquicly identify sector information. It get extra information that will need to be cleaned out \n",
    "      Industry_sector = soup.find_all('div', class_='Bdbw(1px) Bdbc($seperatorColor) Bdbs(s) H(25px) Pt(10px)')\n",
    "\n",
    "      for sector in Industry_sector:\n",
    "\n",
    "                  # return sector name and value\n",
    "                  sec = str(sector.text)\n",
    "\n",
    "                  # seperate the name and value and update the dictionary Industry_sec with the new key-value pair\n",
    "                  Industry_sec.update (split_string_at_first_number(sec))\n",
    "\n",
    "      # build a dataframe using the Industry_sec dictionary\n",
    "      df = pd.DataFrame(Industry_sec, index =[0])\n",
    "      #  df.head()\n",
    "      return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Start the search for my information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many times the returned data frame contains more column than just the industry sectors.\n",
    "# This is a list of the columns that are actually industry sectors\n",
    "industry_sector_col = [\n",
    "      'Fund_Code', \n",
    "      'Basic Materials', \n",
    "      'Consumer Cyclical', \n",
    "      'Financial Services',\n",
    "      'Real Estate', \n",
    "      'Consumer Defensive', \n",
    "      'Healthcare', 'Utilities',\n",
    "      'Communication Services', \n",
    "      'Energy', \n",
    "      'Industrials', \n",
    "      'Technology']\n",
    "\n",
    "# Create an empty data frame with the column headings\n",
    "df_rollup = pd.DataFrame(columns=industry_sector_col)\n",
    "\n",
    "# list of mutual funds that I want to scrape.\n",
    "funds = [\n",
    "      'BDBKX',\n",
    "      'DIA',\n",
    "      'QQQ',\n",
    "      'RERGX',\n",
    "      # 'RWMGX',\n",
    "      # 'VAW',\n",
    "      # 'VBK',\n",
    "      # 'VDC',\n",
    "      # 'VDE',\n",
    "      # 'VEMPX',\n",
    "      # 'VEUSX',\n",
    "      # 'VEXAX',\n",
    "      # 'VFIAX',\n",
    "      # 'VGK',\n",
    "      # 'VHT',\n",
    "      # 'VIGIX',\n",
    "      # 'VIS',\n",
    "      # 'VMVAX',\n",
    "      # 'VNQ',\n",
    "      # 'VOO',\n",
    "      # 'VOT',\n",
    "      # 'VPU',\n",
    "      # 'VTI',\n",
    "      # 'VTIAX',\n",
    "      # 'VTPSX',\n",
    "      # 'VXF',\n",
    "      # 'VXUS',\n",
    "      'WFSPX',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for loop moving through the list of funds\n",
    "for fund in funds:\n",
    "\n",
    "      # print (fund)\n",
    "      # print (url)\n",
    "      dfa = one_return(fund)\n",
    "      dfa.head()\n",
    "\n",
    "      # clean the df\n",
    "      dfa = dfa[industry_sector_col]\n",
    "      # print (dfa)\n",
    "\n",
    "      # merge this data frame with the others\n",
    "      \n",
    "      # df_rollup = [df_rollup, dfa]\n",
    "\n",
    "      df_rollup = pd.concat([df_rollup, dfa])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_return(fund)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rollup.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close down the browser opened by ChromeDriverManager\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data frame as a CSV file\n",
    "\n",
    "# build the path and file name for the CSV file.\n",
    "file_one = os.path.join('.','Resources', 'Industry_Sector_Load.csv')\n",
    "\n",
    "# Write out the file as the CSV file with headers but without an index.\n",
    "df_rollup.to_csv(file_one, index=False, header=True)                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
